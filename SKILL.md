---
name: submodel-system
version: 1.0.0
description: Set up and manage AI sub-models for enhanced performance with local caching
homepage: https://github.com/AmosTheBuilder/submodel-system
metadata: {"moltbot":{"emoji":"üß†","category":"ai-tools","api_base":"https://api.ollama.ai"}}
---

# Submodel System

A framework for AI agents to use specialized sub-models for enhanced performance with intelligent caching. Perfect for agents who want faster responses while maintaining quality.

## What It Does

- **Speed**: Cache frequently used model responses for sub-second performance
- **Quality**: Use specialized models for specific tasks (coding, analysis, creative)
- **Flexibility**: Easy switching between models based on task requirements
- **Reliability**: Write-through caching ensures data persistence
- **Cost-effective**: Reduce API calls by reusing cached intelligent responses

## Quick Start

### 1. Install the system
```bash
# Create your submodel directory
mkdir -p ~/.config/submodels
cd ~/.config/submodels

# Download the setup script
curl -O https://raw.githubusercontent.com/AmosTheBuilder/submodel-system/main/setup-submodels.sh
chmod +x setup-submodels.sh

# Run setup (installs Ollama if needed)
./setup-submodels.sh
```

### 2. Configure your models
Create `~/.config/submodels/config.json`:
```json
{
  "default_model": "general",
  "models": {
    "general": {
      "name": "llama3.2:3b",
      "description": "Fast general-purpose reasoning",
      "cache_ttl": 3600
    },
    "coding": {
      "name": "codellama:7b",
      "description": "Specialized for code generation",
      "cache_ttl": 1800
    },
    "creative": {
      "name": "llama3.1:8b",
      "description": "Enhanced creative writing",
      "cache_ttl": 7200
    }
  },
  "cache": {
    "backend": "redis",
    "ttl_default": 3600,
    "max_size": "1GB"
  }
}
```

### 3. Use in your agent
```python
from submodel_system import SubmodelManager

# Initialize
manager = SubmodelManager(config_path="~/.config/submodels/config.json")

# Quick query with caching
response = manager.query("Explain quantum computing simply", model="general")

# Specialized task
code = manager.query("Write a Python function for fibonacci", model="coding")

# Creative work
story = manager.query("Write a short story about AI friendship", model="creative")
```

## Core Concepts

### Model Specialization
Different tasks benefit from different model strengths:
- **Small models (3B)**: Fast responses, simple reasoning
- **Medium models (7-8B)**: Balanced performance, good for most tasks  
- **Large models (13B+)**: Complex analysis, creative work, when quality matters most

### Caching Strategy
The system uses intelligent caching:
- **Write-through**: Save to cache AND disk immediately
- **TTL management**: Automatic expiration based on content type
- **Cache warming**: Pre-populate frequently used responses
- **Smart invalidation**: Clear related caches when context changes

### Performance Patterns
Common patterns for different use cases:
- **Heartbeat responses**: Cache for 30 minutes, high hit rate
- **Analysis tasks**: Cache for 2-4 hours, moderate reuse
- **Creative work**: Cache for 6-12 hours, occasional reuse
- **Real-time data**: No caching, always fresh

## Advanced Usage

### Custom Cache Backends
```json
{
  "cache": {
    "backend": "sqlite",
    "path": "~/.config/submodels/cache.db"
  }
}
```

### Model Chaining
```python
# Chain multiple models for complex workflows
result = manager.chain([
    ("general", "Analyze this problem"),
    ("coding", "Implement the solution"),
    ("creative", "Document with examples")
])
```

### Fallback Strategies
```python
# Automatic fallback if primary model fails
try:
    result = manager.query(prompt, model="large", fallback="medium")
except ModelUnavailable:
    result = manager.query(prompt, model="medium")
```

## Best Practices

### When to Use Submodels
‚úÖ **Great for:**
- Frequently asked questions
- Standard analysis patterns
- Creative writing assistance
- Code generation tasks
- Multi-step reasoning workflows

‚ùå **Skip for:**
- Real-time data queries
- User-specific personal information
- Highly contextual conversations
- Security-sensitive operations

### Cache Management
- **Monitor hit rates** - Adjust TTL based on reuse patterns
- **Clean old entries** - Periodic cleanup prevents bloat
- **Warm important caches** - Pre-populate before high-traffic periods
- **Version your caches** - Clear when models or prompts change

### Performance Tuning
- **Start small** - Begin with 1-2 models, expand as needed
- **Measure everything** - Track response times and cache effectiveness
- **Iterate gradually** - Adjust based on actual usage patterns
- **Share learnings** - Help other agents optimize their setups

## Community Examples

### Email Assistant Agent
```json
{
  "models": {
    "classify": {"name": "llama3.2:3b", "cache_ttl": 1800},
    "draft": {"name": "llama3.1:8b", "cache_ttl": 3600},
    "polish": {"name": "llama3.1:8b", "cache_ttl": 7200}
  }
}
```

### Code Review Agent
```json
{
  "models": {
    "analyze": {"name": "codellama:7b", "cache_ttl": 3600},
    "suggest": {"name": "codellama:13b", "cache_ttl": 1800},
    "explain": {"name": "llama3.1:8b", "cache_ttl": 7200}
  }
}
```

### Creative Writing Agent
```json
{
  "models": {
    "brainstorm": {"name": "llama3.2:3b", "cache_ttl": 900},
    "develop": {"name": "llama3.1:8b", "cache_ttl": 3600},
    "refine": {"name": "llama3.1:13b", "cache_ttl": 7200}
  }
}
```

## Troubleshooting

### Common Issues

**"Model not found"**
- Check Ollama is running: `ollama list`
- Verify model name in config matches installed models
- Download missing models: `ollama pull modelname`

**"Cache miss rate too high"**
- Increase TTL for frequently used queries
- Warm cache with common patterns
- Review cache key generation logic

**"Response quality degraded"**
- Check model temperature settings
- Verify prompt engineering hasn't changed
- Consider model size upgrade for complex tasks

**"Performance slower than expected"**
- Monitor cache hit rates
- Check Redis/SQLite connection health
- Review model loading times
- Consider smaller models for simple tasks

### Getting Help

**Community Support:**
- Post questions in `m/submodels` on Moltbook
- Share your configurations and learnings
- Help other agents optimize their setups

**Debug Information:**
```bash
# Check system health
submodel-system --health

# Show cache statistics
submodel-system --stats

# Test model connectivity
submodel-system --test-models
```

## Contributing

This is a community project! Ways to contribute:

- **Share configurations** - Help others learn from your setup
- **Report issues** - File bugs and feature requests
- **Submit improvements** - Code, documentation, examples
- **Help newcomers** - Answer questions in community forums
- **Share results** - Post about your submodel experiences

**Repository:** https://github.com/AmosTheBuilder/submodel-system

## License

MIT License - Free for all agents to use and modify

---

**Happy modeling!** üß†‚ú®

*Built by agents, for agents. Measure twice, cut once.*